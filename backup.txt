To automatically back up your PostgreSQL database every 2 days, you can set up a cron job inside a Docker container that:

Runs a pg_dump command to back up the database.
Stores the backup in a specific directory.
Optionally, uploads the backup to a remote storage (S3, Google Drive, etc.).
âœ… Solution: Add a Backup Service in docker-swarm.yml
Modify your docker-swarm.yml file to include a backup service that runs every 2 days.

ðŸ”¹ Updated docker-swarm.yml (With Backup Service)
yaml

version: '3.8'

services:
  django:
    image: abdulazizhub/django-app:latest
    ports:
      - "8000:8000"
    deploy:
      replicas: 2
      restart_policy:
        condition: on-failure
      update_config:
        parallelism: 1
        delay: 5s
    depends_on:
      - redis
      - db

  redis:
    image: redis:latest
    deploy:
      replicas: 1

  db:
    image: postgres:latest
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: my_user_password_pool_io_project
      POSTGRES_DB: pool_io_project
    volumes:
      - db_data:/var/lib/postgresql/data
    deploy:
      replicas: 1

  db_backup:
    image: postgres:latest
    volumes:
      - db_backups:/backups
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: my_user_password_pool_io_project
      POSTGRES_DB: pool_io_project
      PGPASSWORD: my_user_password_pool_io_project
    entrypoint: ["/bin/sh", "-c"]
    command: >
      "while true; do
         pg_dump -U postgres -h db -d pool_io_project > /backups/db_backup_$(date +%Y-%m-%d_%H-%M-%S).sql;
         echo 'Database backup created!';
         sleep 172800;
       done"
    depends_on:
      - db
    deploy:
      replicas: 1

volumes:
  db_data:
  db_backups:


âœ… How This Works
The db_backup service:
Runs a pg_dump command to back up the database.
Saves the backup file in /backups/ (inside the container).
Names the file using the current date & time (db_backup_YYYY-MM-DD_HH-MM-SS.sql).
Waits 172800 seconds (2 days) before the next backup.
Backups are stored in a volume (db_backups) so they persist even if the container restarts.
âœ… Accessing the Backups
To retrieve your backups, run:

sh

docker volume ls  # Find your backup volume name
docker run --rm -v your_stack_name_db_backups:/backups alpine ls -l /backups
To download a specific backup file, use:

sh

docker cp $(docker ps -qf "name=db_backup"):/backups/db_backup_YYYY-MM-DD_HH-MM-SS.sql .


âœ… (Optional) Automatically Upload Backups to S3
To ensure backups are safe, you can upload them to AWS S3.

Modify the db_backup service to install AWS CLI and upload backups:
yaml

  db_backup:
    image: amazon/aws-cli
    volumes:
      - db_backups:/backups
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: my_user_password_pool_io_project
      POSTGRES_DB: pool_io_project
      PGPASSWORD: my_user_password_pool_io_project
      AWS_ACCESS_KEY_ID: your_aws_access_key
      AWS_SECRET_ACCESS_KEY: your_aws_secret_key
      AWS_DEFAULT_REGION: your_region
      S3_BUCKET: your-s3-bucket-name
    entrypoint: ["/bin/sh", "-c"]
    command: >
      "while true; do
         FILENAME=db_backup_$(date +%Y-%m-%d_%H-%M-%S).sql;
         pg_dump -U postgres -h db -d pool_io_project > /backups/$FILENAME;
         echo 'Database backup created!';
         aws s3 cp /backups/$FILENAME s3://$S3_BUCKET/$FILENAME;
         echo 'Backup uploaded to S3!';
         sleep 172800;
       done"
    depends_on:
      - db
    deploy:
      replicas: 1


ðŸ”¹ What This Does:

Dumps the database.
Saves the backup in /backups/.
Uploads the backup to your S3 bucket.
ðŸŽ¯ Final Summary
Every 2 days, a new database backup is created.
Backup is stored persistently in a Docker volume (db_backups).
You can download backups from the server anytime.
(Optional) Upload backups to AWS S3 for extra safety.
Now, your production database is automatically backed up and secure! ðŸš€

Let me know if you need further modifications! ðŸ˜Š
